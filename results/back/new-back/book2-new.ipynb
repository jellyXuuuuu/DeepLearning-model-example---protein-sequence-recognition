{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68978,"databundleVersionId":7709659,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-17T12:02:07.074541Z","iopub.execute_input":"2024-03-17T12:02:07.075461Z","iopub.status.idle":"2024-03-17T12:02:08.467649Z","shell.execute_reply.started":"2024-03-17T12:02:07.075422Z","shell.execute_reply":"2024-03-17T12:02:08.466416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install torch","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:02:08.469714Z","iopub.execute_input":"2024-03-17T12:02:08.470088Z","iopub.status.idle":"2024-03-17T12:02:08.475017Z","shell.execute_reply.started":"2024-03-17T12:02:08.470044Z","shell.execute_reply":"2024-03-17T12:02:08.473965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\n\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport os\n\n# 假设已经定义了一个函数encode_label将标签编码为整数\n\nclass ProteinDataset(Dataset):\n    def __init__(self, sequences_csv, labels_csv, pssm_dir):\n        self.labels_df = pd.read_csv(labels_csv)\n        self.sequences_df = pd.read_csv(sequences_csv)\n        self.pssm_dir = pssm_dir\n#         self.pdb_ids = self.labels_df['PDB_ID'].unique()\n\n\n    def __len__(self):\n        return len(self.sequences_df)\n\n    def __getitem__(self, idx):\n#         pdb_id = self.pdb_ids[idx]\n        pdb_id = self.sequences_df.iloc[idx]['PDB_ID']\n        sequence = self.sequences_df.iloc[idx]['SEQUENCE']\n        label = self.labels_df[self.labels_df['PDB_ID'] == pdb_id]['SEC_STRUCT'].values[0]\n        \n        # 加载PSSM文件\n        pssm_path = os.path.join(self.pssm_dir, f\"{pdb_id}_train.csv\")\n        pssm_df = pd.read_csv(pssm_path, usecols=lambda column : column not in [\"RES_NUM\", \"AMINO_ACID\"])\n        # 转换为数值类型，并填充NaN值\n        pssm_df = pssm_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n        pssm_tensor = torch.tensor(pssm_df.values, dtype=torch.float32).T  # 转置以匹配模型输入 [sequence_length, num_amino_acids]\n\n        # 编码标签\n        label_encoded = encode_label(label)\n        \n        return pssm_tensor, label_encoded\n\n# 一个示例encode_label函数，您需要根据实际标签进行调整\ndef encode_label(label):\n    label_dict = {'H': 0, 'E': 1, 'C': 2}  # 示例的标签字典\n    return torch.tensor([label_dict[aa] for aa in label], dtype=torch.long)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T18:45:09.339760Z","iopub.execute_input":"2024-03-17T18:45:09.340229Z","iopub.status.idle":"2024-03-17T18:45:09.355466Z","shell.execute_reply.started":"2024-03-17T18:45:09.340197Z","shell.execute_reply":"2024-03-17T18:45:09.354067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass FullyConvNet(nn.Module):\n    def __init__(self, num_classes):\n        super(FullyConvNet, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=20, out_channels=64, kernel_size=3, padding=1)\n        # 后续层保持不变...\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.final_conv = nn.Conv1d(256, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        # Assuming `x` is your input tensor with shape [32, 1231, 20]\n        x = x.permute(0, 2, 1)  # Permute to get [batch_size, num_channels, sequence_length]\n\n        # Now, `x` has the shape [32, 20, 1231], which matches the expected input shape of the Conv1D layer\n#         x = F.relu(self.bn1(self.conv1(x)))\n\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.final_conv(x)\n        return F.log_softmax(x, dim=1)  # 使用log_softmax为后续的NLLLoss准备\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T18:45:11.228371Z","iopub.execute_input":"2024-03-17T18:45:11.228816Z","iopub.status.idle":"2024-03-17T18:45:11.242744Z","shell.execute_reply.started":"2024-03-17T18:45:11.228780Z","shell.execute_reply":"2024-03-17T18:45:11.241475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# def calculate_accuracy(predictions, targets):\n#     # Get the predicted class for each sequence\n#     _, predicted_classes = predictions.max(dim=1)\n#     # Compare with the target class\n#     correct = (predicted_classes == targets).sum().item()\n#     # Compute accuracy\n#     accuracy = correct / targets.size(0)\n#     return accuracy\ndef calculate_accuracy(outputs, labels):\n    _, predicted = torch.max(outputs.data, 1)\n    correct = (predicted == labels).sum().item()\n    return correct / labels.size(0)\n\n\n\ndef train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n    train_losses = []\n    train_accuracy = []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        total_correct = 0\n        total_samples = 0\n\n        for sequences, labels in train_loader:\n            sequences, labels = sequences.to(device), labels.to(device)\n            \n            outputs = model(sequences)\n            loss = criterion(outputs, labels)\n            \n            labels_flat = labels.view(-1)\n            output_flat = outputs.view(-1, num_classes)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n#             _, predicted = torch.max(outputs.data, 1)\n#             total_correct += (predicted == labels).sum().item()\n#             total_samples += labels.size(0)\n             # 忽略填充位置后计算准确率\n            _, predicted = torch.max(output_flat, 1)\n            mask = labels_flat != -1  # 创建一个掩码，以忽略填充的位置\n            total_correct += (predicted[mask] == labels_flat[mask]).sum().item()\n            total_samples += mask.sum().item()\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = total_correct / total_samples\n        train_losses.append(epoch_loss)\n        train_accuracy.append(epoch_accuracy)\n        \n        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n    \n    \n    # 绘制损失和准确率曲线\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.title('Loss during training')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accuracy, label='Train Accuracy')\n    plt.title('Accuracy during training')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T18:51:15.283429Z","iopub.execute_input":"2024-03-17T18:51:15.283864Z","iopub.status.idle":"2024-03-17T18:51:15.302793Z","shell.execute_reply.started":"2024-03-17T18:51:15.283833Z","shell.execute_reply":"2024-03-17T18:51:15.301180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim import Adam\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torch.nn.utils.rnn import pad_sequence\nimport matplotlib.pyplot as plt\n\ndef collate_fn(batch):\n    sequences, labels = zip(*batch)\n\n    # Your sequence handling code here...\n    sequences_transposed = [seq.transpose(0, 1) for seq in sequences]  # This might already be correct depending on your data\n    sequences_padded = pad_sequence(sequences_transposed, batch_first=True, padding_value=0)\n\n    # Handling variable-length label sequences\n    # Convert labels to tensors if they aren't already\n#     labels_tensors = [torch.tensor(label, dtype=torch.long) for label in labels]\n    # Adjusted line to create new tensors from existing ones\n    labels_tensors = [label.clone().detach().long() for label in labels]\n\n    # Pad label sequences so they all have the same length\n    labels_padded = pad_sequence(labels_tensors, batch_first=True, padding_value=-1)  # Assuming -1 is an appropriate padding value for your task\n\n    return sequences_padded, labels_padded\n\n\n\n# 假设您已经定义了ProteinDataset类\n\n# 假设您的数据和PSSM文件存放在指定目录下\nsequences_csv = '/kaggle/input/deep-learning-for-msc-202324/seqs_train.csv'\nlabels_csv = '/kaggle/input/deep-learning-for-msc-202324/labels_train.csv'\npssm_dir = '/kaggle/input/deep-learning-for-msc-202324/train/'\n\n# 初始化数据集和数据加载器\ntrain_dataset = ProteinDataset(sequences_csv, labels_csv, pssm_dir)\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n# 现在，在创建DataLoader时使用这个collate_fn\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n\n# 定义设备\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 初始化模型\nnum_classes = 3  # 假设有3个类别：Helix, Sheet, Coil\nmodel = FullyConvNet(num_classes).to(device)  # 这行加在这里\n\n# 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss(ignore_index=-1)\noptimizer = Adam(model.parameters(), lr=0.001)\n\n# 训练模型\nnum_epochs = 5\ntrain_model(model, train_loader, criterion, optimizer, num_epochs, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T18:51:17.138696Z","iopub.execute_input":"2024-03-17T18:51:17.139951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model_and_optimizer(model, optimizer, save_path=\"model_checkpoint.pth\"):\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n    }, save_path)\n    print(f\"Model and optimizer state dicts saved to {save_path}\")\n    \n# 保存模型和优化器的状态\nsave_model_and_optimizer(model, optimizer, \"model_checkpoint.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T18:36:48.570815Z","iopub.status.idle":"2024-03-17T18:36:48.571268Z","shell.execute_reply.started":"2024-03-17T18:36:48.571044Z","shell.execute_reply":"2024-03-17T18:36:48.571062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model_and_optimizer(model, optimizer, load_path=\"model_checkpoint.pth\"):\n    checkpoint = torch.load(load_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    print(f\"Model and optimizer state dicts loaded from {load_path}\")\nnum_classes =3\nmodel = FullyConvNet(num_classes).to(device)  # 重新创建模型实例并移到设备上\noptimizer = optim.Adam(model.parameters(), lr=1e-5)  # 重新创建优化器实例\n\n# 加载模型和优化器状态\ncheckpoint = torch.load(\"/kaggle/working/model_checkpoint.pth\", map_location=device)  # 确保加载到正确的设备\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# 对于优化器，需要手动确保优化器内部状态也在正确的设备上\nfor state in optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.to(device)\n\n# 准备记录损失和准确率\ntrain_losses = []\ntrain_accuracy = []\n\n# 继续训练\nnum_epochs = 100\ntrain_model(model, train_loader, criterion, optimizer, num_epochs, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T18:36:48.576368Z","iopub.status.idle":"2024-03-17T18:36:48.577083Z","shell.execute_reply.started":"2024-03-17T18:36:48.576756Z","shell.execute_reply":"2024-03-17T18:36:48.576786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ray Tune","metadata":{}},{"cell_type":"code","source":"# # !pip install ray[tune]\n# !pip install -U ipywidgets","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:12:08.360242Z","iopub.status.idle":"2024-03-17T12:12:08.360983Z","shell.execute_reply.started":"2024-03-17T12:12:08.360790Z","shell.execute_reply":"2024-03-17T12:12:08.360808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:12:08.361994Z","iopub.status.idle":"2024-03-17T12:12:08.362493Z","shell.execute_reply.started":"2024-03-17T12:12:08.362307Z","shell.execute_reply":"2024-03-17T12:12:08.362324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch_xla\n# import torch_xla.debug.metrics as met\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.utils.utils as xu\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.test.test_utils as test_utils\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:12:08.364236Z","iopub.status.idle":"2024-03-17T12:12:08.364588Z","shell.execute_reply.started":"2024-03-17T12:12:08.364418Z","shell.execute_reply":"2024-03-17T12:12:08.364432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from ray import tune\n# from ray import train  # 导入ray.train\n# import os\n# import torch\n# import torch.nn.functional as F\n# import torch.nn as nn\n# from torch.utils.data import DataLoader\n\n# # 更新的训练函数\n# def train_protein(config):\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n#     model = FullyConvNet(input_channels=41, num_classes=3).to(device)\n#     optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n#     criterion = nn.CrossEntropyLoss()\n    \n#     # 使用ray.train.get_checkpoint()获取检查点\n#     checkpoint = train.get_checkpoint()\n#     if checkpoint:\n#         with checkpoint.as_directory() as checkpoint_dir:\n#             checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pt\")\n#             model_state, optimizer_state = torch.load(checkpoint_path)\n#             model.load_state_dict(model_state)\n#             optimizer.load_state_dict(optimizer_state)\n    \n#     train_loader = DataLoader(train_dataset, batch_size=int(config[\"batch_size\"]), shuffle=True)\n\n#     for epoch in range(10):  # 可以根据需要调整epoch数量\n#         total_loss = 0\n#         model.train()\n#         for sequences, labels in train_loader:\n#             sequences, labels = sequences.to(device), labels.to(device)\n#             optimizer.zero_grad()\n#             output = model(sequences)\n#             loss = criterion(output, labels)\n#             loss.backward()\n#             optimizer.step()\n#             total_loss += loss.item()\n\n#         # 在Ray Tune中报告性能指标\n#         tune.report(loss=total_loss/len(train_loader))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:12:08.365653Z","iopub.status.idle":"2024-03-17T12:12:08.366062Z","shell.execute_reply.started":"2024-03-17T12:12:08.365833Z","shell.execute_reply":"2024-03-17T12:12:08.365847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from ray.tune import CLIReporter\n# from ray.tune.schedulers import ASHAScheduler\n\n# def tune_hyperparameters(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n#     data_dir = os.path.abspath(\"./data\")  # 根据实际情况调整数据目录\n#     config = {\n#         \"lr\": tune.loguniform(1e-4, 1e-1),\n#         \"batch_size\": tune.choice([16, 32, 64, 128])\n#     }\n\n#     scheduler = ASHAScheduler(\n#         metric=\"loss\",\n#         mode=\"min\",\n#         max_t=max_num_epochs,\n#         grace_period=1,\n#         reduction_factor=2)\n    \n#     reporter = CLIReporter(\n#         metric_columns=[\"loss\", \"training_iteration\"])\n    \n#     result = tune.run(\n#         tune.with_parameters(\n#             train_protein,\n#             data_dir=data_dir),\n#         resources_per_trial={\"cpu\": 1, \"gpu\": gpus_per_trial},\n#         config=config,\n#         num_samples=num_samples,\n#         scheduler=scheduler,\n#         progress_reporter=reporter)\n    \n#     best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n#     print(\"Best trial config: {}\".format(best_trial.config))\n#     print(\"Best trial final validation loss: {}\".format(\n#         best_trial.last_result[\"loss\"]))\n    \n#     # 可以根据需要加载最佳模型并进一步处理\n#     # best_trained_model = FullyConvNet(input_channels=41, num_classes=3)\n#     # best_checkpoint_dir = best_trial.checkpoint.value\n#     # checkpoint_path = os.path.join(best_checkpoint_dir, \"checkpoint\")\n#     # model_state, optimizer_state = torch.load(checkpoint_path)\n#     # best_trained_model.load_state_dict(model_state)\n\n# if __name__ == \"__main__\":\n#     # 这里假设你已经定义了train_dataset或其他相关变量\n#     tune_hyperparameters(num_samples=10, max_num_epochs=10, gpus_per_trial=1)\n#     # 使用最佳超参数重新训练模型\n    \n#     best_lr = best_trial.config[\"lr\"]\n#     best_batch_size = best_trial.config[\"batch_size\"]\n\n#     train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n\n#     # 重新初始化模型\n#     model = FullyConvNet(input_channels=41, num_classes=3).to(device)\n#     optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n#     criterion = nn.CrossEntropyLoss()\n\n#     # 重新训练模型\n#     train_model(model, train_loader, criterion, optimizer, num_epochs=10, device=device)\n\n#     # 在这里添加代码以在验证集上评估模型性能\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:12:08.367448Z","iopub.status.idle":"2024-03-17T12:12:08.367794Z","shell.execute_reply.started":"2024-03-17T12:12:08.367626Z","shell.execute_reply":"2024-03-17T12:12:08.367640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PREDICTION","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport csv\n\n# 保留之前定义的encode_amino_acid和encode_sequence函数\n\nclass TestProteinDataset(Dataset):\n    def __init__(self, sequences_csv, test_dir):\n        self.sequences_df = pd.read_csv(sequences_csv)\n        self.pssm_dir = test_dir\n        \n\n    def __getitem__(self, idx):\n        pdb_id = self.sequences_df.iloc[idx]['PDB_ID']\n        sequence = self.sequences_df.iloc[idx]['SEQUENCE']\n        \n        # 加载PSSM文件\n        pssm_path = os.path.join(self.pssm_dir, f\"{pdb_id}_test.csv\")\n        pssm_df = pd.read_csv(pssm_path, usecols=lambda column : column not in [\"RES_NUM\", \"AMINO_ACID\"])\n        # 转换为数值类型，并填充NaN值\n        pssm_df = pssm_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n        pssm_tensor = torch.tensor(pssm_df.values, dtype=torch.float32).T  # 转置以匹配模型输入 [sequence_length, num_amino_acids]\n\n        \n        return pssm_tensor, pdb_id\n    \n    def __len__(self):\n        return len(self.sequences_df)\n\n\ndef predict_collate_fn(batch):\n    sequences, pdb_ids = zip(*batch)\n    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n    return sequences_padded, pdb_ids\n\n\ndef predict(model, loader, device):\n    model.eval()\n    predictions = {}\n\n    with torch.no_grad():\n        for sequences, pdb_ids in loader:\n            sequences = sequences.to(device)\n            sequences = sequences.permute(0, 2, 1)  # 将sequences的维度从[batch_size, sequence_length, channels]转换为[batch_size, channels, sequence_length]\n\n            output = model(sequences)  # 假设输出维度为 (batch_size, num_classes, sequence_length)\n\n            # 由于batch_size=1，直接处理每个batch\n            for i, pdb_id in enumerate(pdb_ids):\n                output_seq = output[i]  # 输出形状应为 (num_classes, sequence_length)\n                _, predicted_seq = torch.max(output_seq, dim=0)  # 对每个位置取最大值获取类别\n                \n                # 保存预测结果\n                if pdb_id not in predictions:\n                    predictions[pdb_id] = []\n                predictions[pdb_id].extend(predicted_seq.cpu().numpy())\n\n    return predictions\n\n# 以下是你之前的代码\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncheckpoint = torch.load('/kaggle/working/model_checkpoint.pth')\nnum_classes = 3\nmodel = FullyConvNet(num_classes).to(device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\ntest_dataset = TestProteinDataset(sequences_csv='/kaggle/input/deep-learning-for-msc-202324/seqs_test.csv', test_dir='/kaggle/input/deep-learning-for-msc-202324/test/')\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=predict_collate_fn)\n\npredictions = predict(model, test_loader, device)\nprint(len(predictions))\n\n\noutput_path = '/kaggle/working/submission.csv'\n\nwith open(output_path, 'w', newline='') as csvfile:\n    csvwriter = csv.writer(csvfile)\n    csvwriter.writerow(['ID', 'STRUCTURE'])\n    for pdb_id, pred_labels in predictions.items():\n        for residue_index, residue_prediction in enumerate(pred_labels):\n            residue_id = f'{pdb_id}_{residue_index + 1}'  # 构建残基ID，索引从1开始\n            prediction_label = 'C' if residue_prediction == 0 else ('H' if residue_prediction == 1 else 'E')\n            # 将每个残基的预测标签写入CSV文件\n            csvwriter.writerow([residue_id, prediction_label])\n\nprint(f\"Predictions have been saved to {output_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T12:12:08.368758Z","iopub.status.idle":"2024-03-17T12:12:08.369196Z","shell.execute_reply.started":"2024-03-17T12:12:08.368931Z","shell.execute_reply":"2024-03-17T12:12:08.368945Z"},"trusted":true},"execution_count":null,"outputs":[]}]}