{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7846308,
          "sourceType": "datasetVersion",
          "datasetId": 4600693
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:34.234123Z",
          "iopub.execute_input": "2024-03-16T01:59:34.234850Z",
          "iopub.status.idle": "2024-03-16T01:59:37.239987Z",
          "shell.execute_reply.started": "2024-03-16T01:59:34.234815Z",
          "shell.execute_reply": "2024-03-16T01:59:37.239059Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.241890Z",
          "iopub.execute_input": "2024-03-16T01:59:37.242619Z",
          "iopub.status.idle": "2024-03-16T01:59:37.247953Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.242586Z",
          "shell.execute_reply": "2024-03-16T01:59:37.247110Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import os\n\n# # Merge sequence data with PSSM profiles\n# def merge_data(seqs_df, data_dir):\n#     merged_data = pd.DataFrame()\n#     for index, row in seqs_df.iterrows():\n#         pdb_id = row['PDB_ID']\n#         file_path = os.path.join(data_dir, f\"{pdb_id}_train.csv\")\n#         if os.path.exists(file_path):  # Check if file exists\n#             train_data = pd.read_csv(file_path)\n#             train_data['PDB_ID'] = pdb_id\n#             merged_data = pd.concat([merged_data, train_data])\n#         else:\n#             print(f\"File not found: {file_path}\")\n#     return merged_data\n\n# # Specify paths for train and test data directories\n# train_data_dir = '/kaggle/input/deep-learning-for-msc/train/'\n# test_data_dir = '/kaggle/input/deep-learning-for-msc/test/'\n\n# # Merge train data\n# train_data = merge_data(seqs_train, train_data_dir)\n# print(\"Shape of merged train data:\", train_data.shape)\n\n# # Merge test data\n# test_data = merge_data(seqs_test, test_data_dir)\n# print(\"Shape of merged test data:\", test_data.shape)\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.249207Z",
          "iopub.execute_input": "2024-03-16T01:59:37.249517Z",
          "iopub.status.idle": "2024-03-16T01:59:37.263157Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.249494Z",
          "shell.execute_reply": "2024-03-16T01:59:37.262305Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## import TPU:\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n\n\n# import TPU package here\n# import torch\n# import torchvision\n# import torchvision.transforms as transforms\n# import torch.nn as nn\n# import torch.optim as optim\n\n# import torch_xla\n# import torch_xla.debug.metrics as met\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.utils.utils as xu\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.test.test_utils as test_utils\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.265157Z",
          "iopub.execute_input": "2024-03-16T01:59:37.265481Z",
          "iopub.status.idle": "2024-03-16T01:59:37.272994Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.265457Z",
          "shell.execute_reply": "2024-03-16T01:59:37.272250Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "'''设计和实现模型：\n根据要求设计并在PyTorch中实现一个全卷积模型，\n用于预测蛋白质的二级结构。\n确保模型能够接受完整的蛋白质序列或PSSM序列作为输入，\n并输出完整的二级结构标签。\n''' \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FullyConvModel(nn.Module):\n    def __init__(self, input_channels, output_channels):\n        super(FullyConvModel, self).__init__()\n        # Define convolutional layers\n        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(in_channels=128, out_channels=output_channels, kernel_size=3, padding=1)\n        \n    def forward(self, x):\n        # Forward pass through convolutional layers\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.conv3(x)  # No activation function on last layer for multi-class classification\n        return x\n\n# Example usage:\ninput_channels = 20  # Assuming input is one-hot encoded amino acid sequence\noutput_channels = 3  # Three output classes: Helix, Sheet, Coil\nmodel = FullyConvModel(input_channels, output_channels)\nprint(model)",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.274152Z",
          "iopub.execute_input": "2024-03-16T01:59:37.274491Z",
          "iopub.status.idle": "2024-03-16T01:59:37.285964Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.274460Z",
          "shell.execute_reply": "2024-03-16T01:59:37.285148Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import optuna\n\n# # Define objective function for optimization\n# def objective(trial):\n#     lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-1)\n#     conv_layers = trial.suggest_int(\"conv_layers\", 1, 5)\n#     kernel_size = trial.suggest_int(\"kernel_size\", 3, 7)\n\n#     # Train and evaluate model with hyperparameters\n#     # Return evaluation metric (e.g., validation accuracy)\n#     return 0.85  # Placeholder value, replace with actual evaluation result\n\n# # Run hyperparameter optimization\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=10)  # Adjust n_trials based on resource constraints\n# best_params = study.best_params\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.286980Z",
          "iopub.execute_input": "2024-03-16T01:59:37.287232Z",
          "iopub.status.idle": "2024-03-16T01:59:37.296282Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.287204Z",
          "shell.execute_reply": "2024-03-16T01:59:37.295274Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "实现超参数优化：使用Ray Tune或Ax实现超参数优化，以寻找最佳的超参数组合。考虑到Kaggle上的有限GPU/TPU资源，选择合适的优化策略。",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import torch\n# from ray import tune\n# from ray.tune.examples.mnist_pytorch import train_mnist\n\n# # Define the training function\n# def train(config):\n#     # Create and train your PyTorch model using the provided configuration\n#     model = FullyConvModel(input_channels, output_channels)\n#     optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n#     # Train the model using the given configuration\n#     ...\n\n# # Define the search space for hyperparameters\n# search_space = {\n#     \"lr\": tune.loguniform(1e-4, 1e-1),\n#     \"conv_layers\": tune.randint(1, 5),\n#     \"kernel_size\": tune.choice([3, 5, 7])\n# }\n\n# # Configure the Ray Tune experiment\n# analysis = tune.run(\n#     train,\n#     config=search_space,\n#     num_samples=10,  # Number of trials\n#     metric=\"mean_accuracy\",  # Name of the metric to optimize\n#     mode=\"max\",  # Maximizing the metric\n#     resources_per_trial={\"gpu\": 1}  # Use 1 GPU per trial\n# )\n\n# # Get the best hyperparameters\n# best_config = analysis.get_best_config(metric=\"mean_accuracy\", mode=\"max\")\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.297738Z",
          "iopub.execute_input": "2024-03-16T01:59:37.298225Z",
          "iopub.status.idle": "2024-03-16T01:59:37.305156Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.298188Z",
          "shell.execute_reply": "2024-03-16T01:59:37.304407Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "optimizer = optim.Adam(model.parameters(), lr=0.001)\noptimizer",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.306128Z",
          "iopub.execute_input": "2024-03-16T01:59:37.306411Z",
          "iopub.status.idle": "2024-03-16T01:59:37.319105Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.306389Z",
          "shell.execute_reply": "2024-03-16T01:59:37.318269Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class CustomDataset(Dataset):\n    def __init__(self, data_dir, sequences_file, labels_file, transform=None):\n        self.data_dir = data_dir\n        self.sequences_df = pd.read_csv(sequences_file)\n        self.labels_df = pd.read_csv(labels_file)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.sequences_df)\n\n    def __getitem__(self, idx):\n        pdb_id = self.sequences_df.iloc[idx]['PDB_ID']\n        sequence = self.load_sequence(pdb_id)\n#         sequence = sequences_df\n        label = self.labels_df[self.labels_df['PDB_ID'] == pdb_id]['SEC_STRUCTURE'].values[0]  # Assuming LABEL is the column name for labels\n        return sequence, label\n\n    def load_sequence(self, pdb_id):\n        sequences_file_path = '/kaggle/input/deep-learning-for-msc/seqs_train.csv'\n        labels_file_path = '/kaggle/input/deep-learning-for-msc/labels_train.csv'\n        # Load sequence data from file\n        # Implement this part according to your data format\n        sequence_data = pd.read_csv(sequence_file_path)  # Load your sequence data\n        return sequence_data\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.320284Z",
          "iopub.execute_input": "2024-03-16T01:59:37.320541Z",
          "iopub.status.idle": "2024-03-16T01:59:37.328700Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.320519Z",
          "shell.execute_reply": "2024-03-16T01:59:37.327963Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data_loader",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.331472Z",
          "iopub.execute_input": "2024-03-16T01:59:37.331745Z",
          "iopub.status.idle": "2024-03-16T01:59:37.343687Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.331720Z",
          "shell.execute_reply": "2024-03-16T01:59:37.342717Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "在训练集上训练模型，并使用验证集评估其性能。跟踪训练过程中的损失和准确率，并绘制损失曲线和准确率曲线。",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from torch.utils.data import Dataset\nimport pandas as pd\n\nclass MyValidationDataset(Dataset):\n    def __init__(self, seqs_file, labels_file, transform=None):\n        self.seqs_df = pd.read_csv(seqs_file)\n        self.labels_df = pd.read_csv(labels_file)\n        self.transform = transform\n        \n        # Optionally, load and preprocess PSSM profiles\n        \n    def __len__(self):\n        return len(self.seqs_df)\n    \n    def __getitem__(self, idx):\n        sequence = self.seqs_df.iloc[idx]['sequence']\n        label = self.labels_df.iloc[idx]['label']\n        \n        # Optionally, apply transformations\n        \n        return sequence, label\n\nval_dataset = MyValidationDataset(seqs_file='/kaggle/input/deep-learning-for-msc/seqs_train.csv', labels_file='/kaggle/input/deep-learning-for-msc/labels_train.csv', transform=None)\nval_dataset",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.344995Z",
          "iopub.execute_input": "2024-03-16T01:59:37.345303Z",
          "iopub.status.idle": "2024-03-16T01:59:37.409911Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.345279Z",
          "shell.execute_reply": "2024-03-16T01:59:37.409028Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from torch.utils.data import DataLoader\n\n# Assuming you have a validation dataset called val_dataset\n# Define the DataLoader for the validation dataset\nbatch_size = 32  # Adjust based on your specific requirements\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\nval_loader",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.410961Z",
          "iopub.execute_input": "2024-03-16T01:59:37.411230Z",
          "iopub.status.idle": "2024-03-16T01:59:37.418002Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.411207Z",
          "shell.execute_reply": "2024-03-16T01:59:37.417011Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# # Define your model architecture\n# class FullyConvModel(nn.Module):\n#     def __init__(self, input_channels, output_channels):\n#         super(FullyConvModel, self).__init__()\n#         # Define convolutional layers\n#         self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n#         self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n#         self.conv3 = nn.Conv1d(128, output_channels, kernel_size=3, padding=1)\n\n#     def forward(self, x):\n#         # Forward pass through convolutional layers\n#         x = F.relu(self.conv1(x))\n#         x = F.relu(self.conv2(x))\n#         x = self.conv3(x)\n#         return x\n\n# # Instantiate the model\n# input_channels = 20  # Assuming input is amino acid sequences\n# output_channels = 3  # Assuming 3 classes for secondary structure prediction (Helix, Sheet, Coil)\n# model = FullyConvModel(input_channels, output_channels)\n\n# # Define your data loaders for training and validation sets\n# # Make sure you have defined the data loaders properly before proceeding\n\n# # Define your loss function and optimizer\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# # Train the model\n# train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, criterion, optimizer, data_loader, val_loader, num_epochs=10)\n\n# # Plot the training and validation curves\n# epochs = range(1, num_epochs+1)\n\n# plt.figure(figsize=(10, 5))\n# plt.subplot(1, 2, 1)\n# plt.plot(epochs, train_losses, label='Train')\n# plt.plot(epochs, val_losses, label='Validation')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.title('Training and Validation Loss')\n# plt.legend()\n\n# plt.subplot(1, 2, 2)\n# plt.plot(epochs, train_accuracies, label='Train')\n# plt.plot(epochs, val_accuracies, label='Validation')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy')\n# plt.title('Training and Validation Accuracy')\n# plt.legend()\n\n# plt.show()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.419275Z",
          "iopub.execute_input": "2024-03-16T01:59:37.419599Z",
          "iopub.status.idle": "2024-03-16T01:59:37.428853Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.419568Z",
          "shell.execute_reply": "2024-03-16T01:59:37.428059Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import os\n# import pandas as pd\n# import torch\n# from torch.utils.data import Dataset, DataLoader\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.nn.utils.rnn import pad_sequence\n\n# class CustomDataset(Dataset):\n#     def __init__(self, data_dir, sequences_file, labels_file, max_sequence_length, transform=None):\n#         self.data_dir = data_dir\n#         self.sequences_df = pd.read_csv(sequences_file)\n#         self.labels_df = pd.read_csv(labels_file)\n#         self.max_sequence_length = max_sequence_length\n#         self.transform = transform\n#         self.label_mapping = self.create_label_mapping()\n\n#     def __len__(self):\n#         return len(self.sequences_df)\n\n#     def __getitem__(self, idx):\n#         pdb_id = self.sequences_df.iloc[idx]['PDB_ID']\n#         sequence = self.load_sequence(pdb_id)\n#         label = self.labels_df[self.labels_df['PDB_ID'] == pdb_id]['SEC_STRUCT'].values[0]\n\n#         if self.transform:\n#             sequence = self.transform(sequence)\n        \n#         # Ensure label is converted to tensor\n#         label = torch.tensor(self.label_mapping[label])  # Convert label to integer tensor\n        \n#         return sequence, label\n    \n#     def create_label_mapping(self):\n#         unique_labels = self.labels_df['SEC_STRUCT'].unique()\n#         label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n#         return label_mapping\n\n#     def load_sequence(self, pdb_id):\n#         sequence_file_path = os.path.join(self.data_dir, f\"train/{pdb_id}_train.csv\")\n\n#         with open(sequence_file_path, 'r') as file:\n#             sequence_data = file.readlines()  # Read all lines\n#         # Concatenate lines to get the sequence\n#         sequence = ''.join(sequence_data[1:])  # Skip the first line which contains headers\n#         sequence = sequence[:self.max_sequence_length]  # Truncate sequence to max_sequence_length\n        \n#         return sequence\n\n\n# # Example model\n# class SimpleModel(nn.Module):\n#     def __init__(self, input_size, hidden_size, output_size):\n#         super(SimpleModel, self).__init__()\n#         self.fc1 = nn.Linear(input_size, hidden_size)\n#         self.fc2 = nn.Linear(hidden_size, output_size)\n\n#     def forward(self, x):\n#         x = torch.relu(self.fc1(x))\n#         x = self.fc2(x)\n#         return x\n\n# # Hyperparameters\n# input_size = 100  # Example input size\n# hidden_size = 64  # Example hidden size\n# output_size = 3   # Example output size (number of classes)\n# learning_rate = 0.001\n# batch_size = 32\n# num_epochs = 10\n\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.429911Z",
          "iopub.execute_input": "2024-03-16T01:59:37.430198Z",
          "iopub.status.idle": "2024-03-16T01:59:37.445894Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.430173Z",
          "shell.execute_reply": "2024-03-16T01:59:37.444974Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "back\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import os\n# import pandas as pd\n# import torch\n# from torch.utils.data import Dataset, DataLoader\n# import torch.nn as nn\n\n# class CustomDataset(Dataset):\n#     def __init__(self, data_dir, sequences_file, labels_file, max_sequence_length, transform=None):\n#         self.data_dir = data_dir\n#         self.sequences_df = pd.read_csv(sequences_file)\n#         self.labels_df = pd.read_csv(labels_file)\n#         self.max_sequence_length = max_sequence_length\n#         self.transform = transform\n#         self.label_mapping = self.create_label_mapping()\n\n#     def __len__(self):\n#         return len(self.sequences_df)\n\n#     def __getitem__(self, idx):\n#         pdb_id = self.sequences_df.iloc[idx]['PDB_ID']\n#         sequence = self.load_sequence(pdb_id)\n#         label = self.labels_df[self.labels_df['PDB_ID'] == pdb_id]['SEC_STRUCT'].values[0]\n\n#         # Convert symbols\n#         sequence = convert_symbols(sequence)\n\n#         if self.transform:\n#             sequence = self.transform(sequence)\n\n#         # Ensure label is converted to tensor\n#         label = torch.tensor(self.label_mapping[label])  # Convert label to integer tensor\n\n#         return sequence, label\n\n#     def create_label_mapping(self):\n#         unique_labels = self.labels_df['SEC_STRUCT'].unique()\n#         label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n#         return label_mapping\n\n#     def load_sequence(self, pdb_id):\n#         sequence_file_path = os.path.join(self.data_dir, f\"train/{pdb_id}_train.csv\")\n\n#         with open(sequence_file_path, 'r') as file:\n#             sequence_data = file.readlines()  # Read all lines\n#         # Concatenate lines to get the sequence\n#         sequence = ''.join(sequence_data[1:])  # Skip the first line which contains headers\n#         sequence = sequence[:self.max_sequence_length]  # Truncate sequence to max_sequence_length\n        \n#         return sequence\n\n# def convert_symbols(sequence):\n#     symbol_mapping = {'H': 'H', 'E': 'E', 'C': 'C'}\n#     return ''.join([symbol_mapping.get(symbol, 'C') for symbol in sequence])\n\n# class SimpleModel(nn.Module):\n#     def __init__(self, input_size, hidden_size, output_size):\n#         super(SimpleModel, self).__init__()\n#         self.fc1 = nn.Linear(input_size, hidden_size)\n#         self.fc2 = nn.Linear(hidden_size, output_size)\n\n#     def forward(self, x):\n#         x = torch.relu(self.fc1(x))\n#         x = self.fc2(x)\n#         return x\n\n# def sequence_to_tensor(sequence, max_sequence_length):\n#     # Split the sequence by comma\n#     numeric_strings = []\n#     for symbol in sequence:\n#         if symbol == 'H':\n#             numeric_strings.append('0')\n#         elif symbol == 'E':\n#             numeric_strings.append('1')\n#         else:\n#             numeric_strings.append('2')\n    \n#     # Convert numeric strings to integers\n#     tensor = torch.tensor([int(s) for s in numeric_strings])\n    \n#     # Pad or truncate sequences to max_sequence_length\n#     if len(tensor) < max_sequence_length:\n#         # Pad sequence with zeros\n#         tensor = torch.cat([tensor, torch.zeros(max_sequence_length - len(tensor), dtype=torch.int)])\n#     elif len(tensor) > max_sequence_length:\n#         # Truncate sequence\n#         tensor = tensor[:max_sequence_length]\n    \n#     return tensor\n\n# def custom_collate(batch):\n#     sequences, labels = zip(*batch)\n#     return torch.stack(sequences), torch.tensor(labels)\n\n# # Hyperparameters\n# input_size = 160  # Set input size to the length of the processed sequences\n# hidden_size = 64\n# output_size = 3\n# learning_rate = 0.001\n# batch_size = 32\n# num_epochs = 10\n\n# # Initialize dataset and data loaders\n# data_dir = \"/kaggle/input/deep-learning-for-msc\"\n# sequences_file_train = os.path.join(data_dir, \"seqs_train.csv\")\n# labels_file_train = os.path.join(data_dir, \"labels_train.csv\")\n# max_sequence_length = 160  # Choose an appropriate maximum sequence length\n\n# custom_dataset_train = CustomDataset(data_dir, sequences_file_train, labels_file_train, max_sequence_length, transform=lambda x: sequence_to_tensor(x, max_sequence_length))\n\n# data_loader_train = DataLoader(custom_dataset_train, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n\n# # Initialize model, loss, and optimizer\n# model = SimpleModel(input_size, hidden_size, output_size)\n# criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# # Training loop\n# for epoch in range(num_epochs):\n#     for batch_idx, (sequences_batch, labels_batch) in enumerate(data_loader_train):\n#         sequences_batch = sequences_batch.float()  # Ensure sequences_batch is converted to float\n        \n#         outputs = model(sequences_batch)\n#         loss = criterion(outputs, labels_batch)\n        \n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n    \n#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.447102Z",
          "iopub.execute_input": "2024-03-16T01:59:37.447379Z",
          "iopub.status.idle": "2024-03-16T01:59:37.462091Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.447356Z",
          "shell.execute_reply": "2024-03-16T01:59:37.461261Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Common imports\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport seaborn as sns\n\nuse_cuda = True\ndevice = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n\nprint(\"Using CUDA ? \" + str(use_cuda))",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T02:05:29.475467Z",
          "iopub.execute_input": "2024-03-16T02:05:29.476162Z",
          "iopub.status.idle": "2024-03-16T02:05:29.591077Z",
          "shell.execute_reply.started": "2024-03-16T02:05:29.476127Z",
          "shell.execute_reply": "2024-03-16T02:05:29.590154Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nclass CustomDataset(Dataset):\n    def __init__(self, data_dir, sequences_file, labels_file, max_sequence_length, transform=None):\n        self.data_dir = data_dir\n        self.sequences_df = pd.read_csv(sequences_file)\n        self.labels_df = pd.read_csv(labels_file)\n        self.max_sequence_length = max_sequence_length\n        self.transform = transform\n        self.label_mapping = self.create_label_mapping()\n\n    def __len__(self):\n        return len(self.sequences_df)\n\n    def __getitem__(self, idx):\n        pdb_id = self.sequences_df.iloc[idx]['PDB_ID']\n        sequence = self.load_sequence(pdb_id)\n        label = self.labels_df[self.labels_df['PDB_ID'] == pdb_id]['SEC_STRUCT'].values[0]\n\n        # Convert symbols\n        sequence = convert_symbols(sequence)\n\n        if self.transform:\n            sequence = self.transform(sequence)\n\n        # Ensure label is converted to tensor\n        label = torch.tensor(self.label_mapping[label])  # Convert label to integer tensor\n\n        return sequence, label\n\n    def create_label_mapping(self):\n        # 统计不同类别的数量\n        label_counts = self.labels_df['SEC_STRUCT'].value_counts()\n\n        # 根据数量最多的类别设置标签映射\n        most_common_labels = label_counts.idxmax()\n        label_mapping = {label: 0 if label == most_common_labels else (1 if label_counts[most_common_labels] == label_counts[label] else 2) for label in label_counts.keys()}\n\n        return label_mapping\n\n    def load_sequence(self, pdb_id):\n        sequence_file_path = os.path.join(self.data_dir, f\"train/{pdb_id}_train.csv\")\n\n        with open(sequence_file_path, 'r') as file:\n            sequence_data = file.readlines()  # Read all lines\n        # Concatenate lines to get the sequence\n        sequence = ''.join(sequence_data[1:])  # Skip the first line which contains headers\n        sequence = sequence[:self.max_sequence_length]  # Truncate sequence to max_sequence_length\n        \n        return sequence\n\ndef convert_symbols(sequence):\n    symbol_mapping = {'H': 'H', 'E': 'E', 'C': 'C'}\n    return ''.join([symbol_mapping.get(symbol, 'C') for symbol in sequence])\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ndef sequence_to_tensor(sequence, max_sequence_length):\n    # Split the sequence by comma\n    numeric_strings = []\n    for symbol in sequence:\n        if symbol == 'H':\n            numeric_strings.append('0')\n        elif symbol == 'E':\n            numeric_strings.append('1')\n        else:\n            numeric_strings.append('2')\n    \n    # Convert numeric strings to integers\n    tensor = torch.tensor([int(s) for s in numeric_strings])\n    \n    # Pad or truncate sequences to max_sequence_length\n    if len(tensor) < max_sequence_length:\n        # Pad sequence with zeros\n        tensor = torch.cat([tensor, torch.zeros(max_sequence_length - len(tensor), dtype=torch.int)])\n    elif len(tensor) > max_sequence_length:\n        # Truncate sequence\n        tensor = tensor[:max_sequence_length]\n    \n    return tensor\n\ndef custom_collate(batch):\n    sequences, labels = zip(*batch)\n    return torch.stack(sequences), torch.tensor(labels)\n\n# Hyperparameters\ninput_size = 160  # Set input size to the length of the processed sequences\nhidden_size = 64\nlearning_rate = 0.001\nbatch_size = 32\nnum_epochs = 50\n\n# Initialize dataset and data loaders\ndata_dir = \"/kaggle/input/deep-learning-for-msc\"\nsequences_file_train = os.path.join(data_dir, \"seqs_train.csv\")\nlabels_file_train = os.path.join(data_dir, \"labels_train.csv\")\nmax_sequence_length = 160  # Choose an appropriate maximum sequence length\n\ncustom_dataset_train = CustomDataset(data_dir, sequences_file_train, labels_file_train, max_sequence_length, transform=lambda x: sequence_to_tensor(x, max_sequence_length))\n\ndata_loader_train = DataLoader(custom_dataset_train, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n\n# 根据标签映射设置输出大小\noutput_size = 3  # 由于有三个类别，所以输出大小为3\n\n# Initialize model, loss, and optimizer\nmodel = SimpleModel(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch_idx, (sequences_batch, labels_batch) in enumerate(data_loader_train):\n        sequences_batch = sequences_batch.float()  # Ensure sequences_batch is converted to float\n        \n        outputs = model(sequences_batch)\n        loss = criterion(outputs, labels_batch)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T01:59:37.463057Z",
          "iopub.execute_input": "2024-03-16T01:59:37.463295Z",
          "iopub.status.idle": "2024-03-16T02:03:14.860789Z",
          "shell.execute_reply.started": "2024-03-16T01:59:37.463274Z",
          "shell.execute_reply": "2024-03-16T02:03:14.859788Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# 保存模型参数\ntorch.save(model.state_dict(), \"trained_model.pth\")",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T02:03:14.862051Z",
          "iopub.execute_input": "2024-03-16T02:03:14.862346Z",
          "iopub.status.idle": "2024-03-16T02:03:14.867974Z",
          "shell.execute_reply.started": "2024-03-16T02:03:14.862319Z",
          "shell.execute_reply": "2024-03-16T02:03:14.866861Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nclass CustomDataset(Dataset):\n    def __init__(self, data_dir, sequences_file, max_sequence_length, transform=None):\n        self.data_dir = data_dir\n        self.sequences_df = pd.read_csv(sequences_file)\n        self.max_sequence_length = max_sequence_length\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.sequences_df)\n\n    def __getitem__(self, idx):\n        pdb_id = self.sequences_df.iloc[idx]['PDB_ID']\n        sequence = self.load_sequence(pdb_id)\n\n        # Convert symbols\n        sequence = convert_symbols(sequence)\n\n        if self.transform:\n            sequence = self.transform(sequence)\n\n        return sequence, pdb_id\n\n    def load_sequence(self, pdb_id):\n        sequence_file_path = os.path.join(self.data_dir, f\"test/{pdb_id}_test.csv\")\n\n        with open(sequence_file_path, 'r') as file:\n            sequence_data = file.readlines()  # Read all lines\n        # Concatenate lines to get the sequence\n        sequence = ''.join(sequence_data[1:])  # Skip the first line which contains headers\n        sequence = sequence[:self.max_sequence_length]  # Truncate sequence to max_sequence_length\n        \n        return sequence\n\ndef convert_symbols(sequence):\n    symbol_mapping = {'H': 'H', 'E': 'E', 'C': 'C'}\n    return ''.join([symbol_mapping.get(symbol, 'C') for symbol in sequence])\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ndef sequence_to_tensor(sequence, max_sequence_length):\n    # Split the sequence by comma\n    numeric_strings = []\n    for symbol in sequence:\n        if symbol == 'H':\n            numeric_strings.append('0')\n        elif symbol == 'E':\n            numeric_strings.append('1')\n        else:\n            numeric_strings.append('2')\n    \n    # Convert numeric strings to integers\n    tensor = torch.tensor([int(s) for s in numeric_strings])\n    \n    # Pad or truncate sequences to max_sequence_length\n    if len(tensor) < max_sequence_length:\n        # Pad sequence with zeros\n        tensor = torch.cat([tensor, torch.zeros(max_sequence_length - len(tensor), dtype=torch.int)])\n    elif len(tensor) > max_sequence_length:\n        # Truncate sequence\n        tensor = tensor[:max_sequence_length]\n    \n    return tensor\n\ndef predict_on_test_set(model, data_loader):\n    predictions = []\n    for sequences_batch, pdb_ids in data_loader:\n        sequences_batch = sequences_batch.float()  # Ensure sequences_batch is converted to float\n        outputs = model(sequences_batch)\n        _, predicted_labels = torch.max(outputs, 1)\n        predictions.extend(zip(pdb_ids, predicted_labels.tolist()))\n    return predictions\n\n# Hyperparameters\ninput_size = 160  # Set input size to the length of the processed sequences\nhidden_size = 64\nbatch_size = 32\n\n# Initialize dataset and data loader for test set\ndata_dir = \"/kaggle/input/deep-learning-for-msc\"\nsequences_file_test = os.path.join(data_dir, \"seqs_test.csv\")\nmax_sequence_length = 160  # Choose an appropriate maximum sequence length\n\ncustom_dataset_test = CustomDataset(data_dir, sequences_file_test, max_sequence_length, transform=lambda x: sequence_to_tensor(x, max_sequence_length))\ndata_loader_test = DataLoader(custom_dataset_test, batch_size=batch_size, shuffle=False)\n\n# Initialize model\noutput_size = 3  # Output size is 3 since we have three classes\nmodel = SimpleModel(input_size, hidden_size, output_size)\n\n# Load trained model weights\nmodel_weights_path = \"/kaggle/working/trained_model.pth\"\nmodel.load_state_dict(torch.load(model_weights_path))\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Make predictions on the test set\npredictions = predict_on_test_set(model, data_loader_test)\n\n# Print the first few predictions\nfor prediction in predictions[:20]:\n    print(\"PDB ID:\", prediction[0], \"Predicted Label:\", prediction[1])\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T02:03:14.869223Z",
          "iopub.execute_input": "2024-03-16T02:03:14.869491Z",
          "iopub.status.idle": "2024-03-16T02:03:15.199018Z",
          "shell.execute_reply.started": "2024-03-16T02:03:14.869468Z",
          "shell.execute_reply": "2024-03-16T02:03:15.198037Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# 在训练数据集中查看标签转换情况\nlabel_mapping = custom_dataset_train.label_mapping\nprint(\"标签映射字典:\", label_mapping)\n\n# 输出一些样本的原始标签和转换后的整数值\nfor i in range(5):  # 输出前5个样本\n    sequence, label = custom_dataset_train[i]\n    original_label = custom_dataset_train.labels_df.iloc[i]['SEC_STRUCT']\n    print(f\"原始标签: {original_label}, 转换后的整数值: {label}\")\n\n# 可视化标签分布\nimport matplotlib.pyplot as plt\n\n# 统计每个类别的样本数量\nlabel_counts = custom_dataset_train.labels_df['SEC_STRUCT'].value_counts()\n\n# 绘制条形图\nplt.figure(figsize=(8, 6))\nplt.bar(label_counts.index, label_counts.values)\nplt.xlabel('标签')\nplt.ylabel('样本数量')\nplt.title('标签分布')\nplt.xticks(rotation=45)  # 旋转标签，避免重叠\nplt.show()\n",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "execution": {
          "iopub.status.busy": "2024-03-16T02:03:15.200320Z",
          "iopub.execute_input": "2024-03-16T02:03:15.200906Z",
          "iopub.status.idle": "2024-03-16T02:05:27.528795Z",
          "shell.execute_reply.started": "2024-03-16T02:03:15.200875Z",
          "shell.execute_reply": "2024-03-16T02:05:27.527005Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import pandas as pd\n\n# # 假设你已经加载了标签数据到 DataFrame 中\n# labels_df = pd.read_csv(\"/kaggle/input/deep-learning-for-msc/labels_train.csv\")\n\n# # 获取标签列的最小值和最大值\n# min_label = labels_df['SEC_STRUCT'].min()\n# max_label = labels_df['SEC_STRUCT'].max()\n\n# print(\"最小标签值:\", min_label)\n# print(\"最大标签值:\", max_label)\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T02:05:27.529822Z",
          "iopub.status.idle": "2024-03-16T02:05:27.530171Z",
          "shell.execute_reply.started": "2024-03-16T02:05:27.529999Z",
          "shell.execute_reply": "2024-03-16T02:05:27.530014Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}